import os
import torch
import torch.nn.functional as F
import torchaudio

from ESN import ESN  # è¯·ç¡®ä¿ ESN ç±»å®ç°æ­£ç¡®ä¸”å¯å¯¼å…¥
from dataset import TidigitDataset, collate_fn  # è¯·ç¡®ä¿æ•°æ®é›†ç±»å’Œ collate_fn å¯å¯¼å…¥
from config import (
    target_sample_rate,
    RAW_TEST_FOLDER_ABS_PATH,
)  # é‡‡æ ·ç‡åŠæµ‹è¯•æ•°æ®é›†è·¯å¾„
from utils import get_caption  # è·å–çœŸå®å­—å¹•çš„è¾…åŠ©å‡½æ•°

# ---------------------------
# è®¾å¤‡ä¸æ¨¡å‹ç›¸å…³è®¾ç½®
# ---------------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_PATH = "best_model.pth"

if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ {MODEL_PATH} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")

# åŠ è½½æ¨¡å‹ï¼ˆä½ ä¹Ÿå¯ä»¥è®¾ç½® weights_only=True æ¥æé«˜å®‰å…¨æ€§ï¼‰
checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
model = ESN(input_size=13, output_size=36, spectral_radius=0.6).to(DEVICE)
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()

# ---------------------------
# è½½å…¥æµ‹è¯•æ•°æ®é›†
# ---------------------------
test_dataset = TidigitDataset(RAW_TEST_FOLDER_ABS_PATH)
# æ³¨æ„ï¼šcollate_fn è¿”å›çš„æ ¼å¼ä¸º (mfccs, labels, mfcc_lengths, paths, captions)
test_dataloader = torch.utils.data.DataLoader(
    test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn
)


# ---------------------------
# CTC è§£ç å‡½æ•°
# ---------------------------
def ctc_decode(seq, blank=0):
    """
    å¯¹é¢„æµ‹çš„ token åºåˆ—è¿›è¡Œ CTC è§£ç ï¼š
      - æŠ˜å è¿ç»­é‡å¤çš„ token
      - ç§»é™¤ç©ºç™½ tokenï¼ˆå³ blankï¼‰
    å‚æ•°ï¼š
      seq: listï¼Œé¢„æµ‹å‡ºçš„ token åºåˆ—ï¼ˆä¾‹å¦‚ [0, 0, 3, 3, 0, 2, ...]ï¼‰
      blank: ç©ºç™½ token çš„ç´¢å¼•ï¼ˆé»˜è®¤ 0ï¼‰
    è¿”å›ï¼š
      è§£ç åçš„ token åˆ—è¡¨ï¼ˆä¾‹å¦‚ [3, 2, ...]ï¼‰
    """
    new_seq = []
    prev = None
    for token in seq:
        # è‹¥å½“å‰ token ä¸ºç©ºç™½ï¼Œç›´æ¥æ›´æ–° prev å¹¶è·³è¿‡
        if token == blank:
            prev = token
            continue
        if token != prev:
            new_seq.append(token)
        prev = token
    return new_seq


# ---------------------------
# æ‰¹é‡é¢„æµ‹ï¼ˆæµ‹è¯•æ•°æ®é›†ï¼‰
# ---------------------------
def predict():
    print("\nğŸš€ æ­£åœ¨å¯¹æµ‹è¯•æ•°æ®é›†è¿›è¡Œé¢„æµ‹...\n")
    with torch.no_grad():
        for mfccs, labels, _, paths, captions in test_dataloader:
            mfccs = mfccs.to(DEVICE)
            # å‰å‘ä¼ æ’­å¾—åˆ°è¾“å‡º shape: (batch_size, T, vocab_size)
            outputs = model(mfccs)
            # è½¬ç½®ä¸º (T, batch_size, vocab_size)
            outputs = outputs.permute(1, 0, 2)
            # å¦‚æœéœ€è¦ï¼Œå¯ä»¥å…ˆåš log_softmaxï¼Œä½†å¯¹ argmax å¹¶ä¸å½±å“
            # outputs = F.log_softmax(outputs, dim=2)
            predicted_ids = torch.argmax(outputs, dim=2)  # shape: (T, batch_size)

            predicted_texts = []
            for i in range(predicted_ids.shape[1]):  # éå† batch å†…æ¯ä¸ªæ ·æœ¬
                raw_seq = predicted_ids[:, i].tolist()
                # ä½¿ç”¨ CTC è§£ç ï¼šæŠ˜å é‡å¤å¹¶ç§»é™¤ç©ºç™½ tokenï¼ˆé»˜è®¤ blank=0ï¼‰
                decoded_seq = ctc_decode(raw_seq, blank=0)
                # è°ƒç”¨å­—ç¬¦è§£ç å™¨å°† token åºåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                predicted_text = test_dataset.caption_encoder.decode(decoded_seq)
                predicted_texts.append(predicted_text)

            # è¾“å‡ºè¯¥ batch çš„ç»“æœï¼ˆè¿™é‡Œåªå±•ç¤ºä¸€ä¸ª batchï¼‰
            for i in range(len(predicted_texts)):
                print(f"ğŸ¯ çœŸå®å­—å¹•: {captions[i]}")
                print(f"ğŸ¤– é¢„æµ‹å­—å¹•: {predicted_texts[i]}")
                print(f"ğŸ“‚ æ–‡ä»¶è·¯å¾„: {paths[i]}")
                print("-" * 40)
            break  # åªå¤„ç†ä¸€ä¸ª batchï¼Œé˜²æ­¢è¾“å‡ºè¿‡å¤š


# ---------------------------
# å•ä¸ªæ–‡ä»¶é¢„æµ‹
# ---------------------------
def predict_single(file_path):
    print(f"\nğŸš€ æ­£åœ¨å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œé¢„æµ‹: {file_path}\n")
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")
    with torch.no_grad():
        # è¯»å–éŸ³é¢‘
        waveform, sample_rate = torchaudio.load(file_path, normalize=True)
        # è‹¥ä¸ºå¤šé€šé“ï¼Œå–å¹³å‡è½¬æ¢ä¸ºå•å£°é“
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        # è‹¥é‡‡æ ·ç‡ä¸ç¬¦ï¼Œåˆ™è¿›è¡Œé‡é‡‡æ ·
        if sample_rate != target_sample_rate:
            waveform = torchaudio.transforms.Resample(
                orig_freq=sample_rate, new_freq=target_sample_rate
            )(waveform)
        # è®¡ç®— MFCC ç‰¹å¾ï¼Œè¾“å‡ºå½¢çŠ¶ (T, n_mfcc)
        transform_mfcc = torchaudio.transforms.MFCC(
            sample_rate=target_sample_rate, n_mfcc=13
        )
        mfcc = transform_mfcc(waveform).squeeze(0).transpose(0, 1)
        mfcc = mfcc.unsqueeze(0).to(DEVICE)  # shape: (1, T, n_mfcc)

        outputs = model(mfcc)  # shape: (1, T, vocab_size)
        outputs = outputs.permute(1, 0, 2)  # shape: (T, 1, vocab_size)
        predicted_ids = torch.argmax(outputs, dim=2)  # shape: (T, 1)
        raw_seq = predicted_ids[:, 0].tolist()
        decoded_seq = ctc_decode(raw_seq, blank=0)
        predicted_text = test_dataset.caption_encoder.decode(decoded_seq)

        real_caption = get_caption(file_path)
        print(f"ğŸ¯ çœŸå®å­—å¹•: {real_caption if real_caption else 'æœªçŸ¥'}")
        print(f"ğŸ¤– é¢„æµ‹å­—å¹•: {predicted_text}")
        print(f"ğŸ“‚ æ–‡ä»¶è·¯å¾„: {file_path}")
        print("-" * 40)


# ---------------------------
# ä¸»å‡½æ•°ï¼šè¿è¡Œé¢„æµ‹
# ---------------------------
if __name__ == "__main__":
    # å…ˆå¯¹æ•´ä¸ªæµ‹è¯•æ•°æ®é›†è¿›è¡Œé¢„æµ‹
    predict()

    # å†å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œé¢„æµ‹ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„ï¼‰
    # test_file = "æµ‹è¯•éŸ³é¢‘æ–‡ä»¶è·¯å¾„.wav"  # æ›¿æ¢ä¸ºä½ çš„æµ‹è¯•æ–‡ä»¶è·¯å¾„
    # predict_single(test_file)
